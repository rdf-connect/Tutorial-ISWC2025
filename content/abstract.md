## Abstract
<!-- Context -->
[RDF-Connect](https://github.com/rdf-connect/) is a novel, language-agnostic framework for building provenance-aware,
streaming data pipelines that integrate heterogeneous processors across programming languages. It aims on facilitating the construction, maintenance and reusability of modular and interoperable pipelines, supporting complex and semantically rich data workflows.
<!-- Need -->
Data processing pipelines are a crucial component of any data-centric system today,
including knowledge graph, LLM or in general, machine learning-based systems. 
Developers and researchers require flexible, interoperable tools for creating multilingual data processing pipelines.
<!-- Task -->
To meet this need, we present a comprehensive, tutorial that blends conceptual foundations with hands-on
experience. Through a series of guided tasks, participants will learn how to use RDF-Connect to design and execute
streaming pipelines that are reusable, extensible and transparent.
<br class='screen-only' />
<!-- Object -->
Participants will construct a streaming data processing pipeline based on a real-world data: generating a knowledge
graph from the Japan Meteorological Agency’s weather forecasts for Nara, Japan. They will: (i) Construct a machine
learning pipeline using processors in multiple programming languages, (ii) Create custom data processors for diverse
endpoints, (iii) Explore provenance tracking using RDF and PROV-O ontology.
<!-- Conclusion -->
By the end of the tutorial, participants from varied backgrounds—including Python, JavaScript, and Java developers—will
gain practical experience in building language-agnostic, semantically rich data processing pipelines.
<!-- Perspectives -->
This tutorial not only introduces RDF-Connect but also opens new avenues for interdisciplinary data transformation
strategies in Semantic Web research and development.
