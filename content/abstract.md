## Abstract
<!-- Context -->
[RDF Connect](https://github.com/rdf-connect/) is a novel, language-agnostic framework for building provenance-aware,
streaming data pipelines that integrate heterogeneous processors across programming languages. It addresses a
longstanding gap in the Semantic Web ecosystem: the lack of modular, interoperable tools for creating complex,
semantically rich data workflows.
<!-- Need -->
In an era of increasingly complex data ecosystems and the growing importance of Large Language Models (LLMs), developers
and researchers require flexible, interoperable tools for creating multilingual data processing pipelines.
<!-- Task -->
To meet this need, we present a comprehensive, full-day tutorial that blends conceptual foundations with hands-on
experience. Through a series of guided tasks, participants will learn to use RDF Connect to design and execute
streaming pipelines that are standards-compliant, extensible, and transparent.
<br class='screen-only' />
<!-- Object -->
Participants will construct a streaming data processing pipeline based on a real-world use case: generating a knowledge
graph from the Japan Meteorological Agency’s weather forecasts for Nara, Japan. They will: (i) Construct a machine
learning pipeline using processors in multiple programming languages, (ii) Create custom data processors for diverse
endpoints, (iii) Explore provenance tracking using RDF and PROV-O ontology.
<!-- Conclusion -->
By the end of the tutorial, participants from varied backgrounds—including Python, JavaScript, and Java developers—will
gain practical experience in building language-agnostic, semantically rich data processing pipelines.
<!-- Perspectives -->
This tutorial not only introduces RDF Connect but also opens new avenues for interdisciplinary data transformation
strategies in Semantic Web research and development.
